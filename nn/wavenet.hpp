#ifndef _NN_CONV_HPP_
#define _NN_CONV_HPP_

#include <vector>
#include "lr.hpp"

/*
def forward(self, x):
    out = x
    skips = []
    out = self.input_layer(out)

    for hidden, residual in zip(self.hidden, self.residuals):
        x = out
        out_hidden = hidden(x)

        # gated activation
        #   split (32,16,3) into two (16,16,3) for tanh and sigm calculations
        out_hidden_split = torch.split(out_hidden, self.num_channels, dim=1)
        out = torch.tanh(out_hidden_split[0]) * torch.sigmoid(out_hidden_split[1])

        skips.append(out)

        out = residual(out)
        out = out + x[:, :, -out.size(2) :]

    # modified "postprocess" step:
    out = torch.cat([s[:, :, -out.size(2) :] for s in skips], dim=1)
    out = self.linear_mix(out)
    return out
*/

namespace lr { namespace nn { namespace wavenet {


struct WaveNetWord : public lr::NativeWord {
    WaveNetWord() {

    }
    virtual ~WaveNetWord() {

    }
    virtual void run(Stack& stack) {

    }
private:
};


}}}


#endif
